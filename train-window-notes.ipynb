{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding window CNN for predicting notes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import IPython.display as ipydisplay\n",
    "import functools\n",
    "import librosa\n",
    "import librosa.display as ldisplay\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import scipy.io.wavfile as wav\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import dataset, bincounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dataloader(folder_path, crop_len_sec=2, sample=False):\n",
    "    _dataset = dataset.SignalWindowDataset(folder_path=folder_path, crop_len_sec=crop_len_sec)\n",
    "\n",
    "    sampler = None\n",
    "    if sample:\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(weights=_dataset.sampling_weights, num_samples=1, replacement=True)\n",
    "\n",
    "    _dataloader = DataLoader(_dataset, batch_size=1, sampler=sampler)\n",
    "    print(len(_dataset), len(_dataloader))\n",
    "    return _dataset, _dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataloader_train = get_dataset_dataloader(folder_path='/home/anuj/data/m/disk/train/', sample=True)\n",
    "dataset_val, dataloader_val = get_dataset_dataloader(folder_path='/home/anuj/data/m/disk/val', sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the sampling\n",
    "# ix = 0\n",
    "# file_paths = []\n",
    "# while ix < 1000:\n",
    "#     batch = next(iter(dataloader_train))\n",
    "#     file_paths.append(batch['file_path'])\n",
    "#     ix += 1\n",
    "# dataset_train.df_stats[['file_path', 'seconds']].sort_values(['seconds'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bincounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weights = bincounts.get_bin_counts(dataloader_train, keys=['labels'], n_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weights['labels'], 'x-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.frame_cnn import SimpleFrameCNNWithNotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFrameCNNWithNotes(n_feats=513).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train))\n",
    "print(batch['features'].shape, batch['labels'].shape)\n",
    "print(batch['labels'])\n",
    "\n",
    "inputs = batch['features'].to(DEVICE)\n",
    "onset_probs, notes_activations = model(inputs)\n",
    "assert np.all(onset_probs.shape[1:] == np.array([2, 1, inputs.shape[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss / optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_l = Variable(torch.from_numpy(weights['labels'].astype(np.float32)))\n",
    "onset_loss_func = torch.nn.NLLLoss(weight=weights_l, ignore_index=-100).to(DEVICE)\n",
    "notes_loss_func = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_str = 'docmus-with-notes-all-1.00'\n",
    "\n",
    "# logging\n",
    "weights_folder = \"../weights/{}\".format(model_str)\n",
    "log_folder =  '../tensorboard-logs/{}'.format(model_str)\n",
    "writer = SummaryWriter(log_folder) # writing log to tensorboard\n",
    "print('logging to: {}'.format(weights_folder))\n",
    "\n",
    "os.makedirs(weights_folder, exist_ok=False)  # MEANT TO FAIL IF IT ALREADY EXISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = collections.namedtuple('Results', ['onset_loss', 'notes_loss', 'precision', 'recall', 'f1', 'support', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onset_times_from_window_labels(onset_windows, window_size, sr):\n",
    "    seconds_in_window = window_size / sr\n",
    "    pred_onsets = np.where(onset_windows.squeeze() == 1)[0] * seconds_in_window + seconds_in_window / 2.\n",
    "    pred_onsets = np.unique(np.round(pred_onsets, decimals=2))\n",
    "    return pred_onsets\n",
    "\n",
    "\n",
    "def predict_and_evalaute(batch, model, onset_loss_func, notes_loss_func, device, visualize=False):\n",
    "    inputs = batch['features'].to(device)\n",
    "    target_labels, target_notes = batch['labels'].to(device), batch['notes'].to(device)\n",
    "\n",
    "    # Predict\n",
    "    onset_probs, notes_activations = model(inputs)\n",
    "    assert np.all(onset_probs.shape[1:] == np.array([2, 1, inputs.shape[-1]]))\n",
    "    pred_onsets = torch.argmax(onset_probs, dim=1)\n",
    "    \n",
    "    # loss\n",
    "    onset_loss = onset_loss_func(onset_probs, target_labels)\n",
    "    notes_loss = notes_loss_func(notes_activations.squeeze(), target_notes.squeeze())\n",
    "\n",
    "    pred_onsets = pred_onsets.data.cpu().numpy()\n",
    "    target_labels = target_labels.data.cpu().numpy()\n",
    "\n",
    "    p, r, f, s = precision_recall_fscore_support(target_labels.squeeze(), pred_onsets.squeeze(), labels=[0, 1])\n",
    "    results = Results(onset_loss=onset_loss, notes_loss=notes_loss, precision=p.mean(), recall=r.mean(), f1=f.mean(), support=s)\n",
    "\n",
    "    if visualize:\n",
    "        pred_onsets = get_onset_times_from_window_labels(pred_onsets, 1024, batch['sr'].data.numpy()[0])\n",
    "        plot_preds_gt(batch, pred_onsets)\n",
    "\n",
    "    return pred_onsets, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(signal, sr, spec, target_onsets, pred_onsets ,figsize=(40, 15)):\n",
    "    # Calculate stuff\n",
    "    duration = signal.shape[0] / sr\n",
    "    n_segments = spec.shape[1]\n",
    "    segment_starts_in_s = np.linspace(0, duration, n_segments + 1)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Plot the signal\n",
    "    plt.subplot(2, 1, 1)\n",
    "    ldisplay.waveplot(signal, sr=sr)\n",
    "\n",
    "    ymax, ymin = max(signal) + 0.05, min(signal) - 0.05\n",
    "    plt.vlines(target_onsets, ymin=ymin, ymax=ymax, colors='g', linestyles='--', linewidths=3)  # plot the signal\n",
    "    plt.vlines(segment_starts_in_s, ymax=ymax, ymin=ymin, colors='gray', linestyles='--', linewidths=1)  # Plot the segment lines\n",
    "    plt.vlines(pred_onsets, ymax=ymax, ymin=ymin, colors='r', linestyles='--', linewidths=2)  # Plot the pred onset lines\n",
    "\n",
    "    # Plot the spectrum\n",
    "    plt.subplot(2, 1, 2)\n",
    "    ldisplay.specshow(librosa.amplitude_to_db(spec), sr=sr, x_axis='time', y_axis='hz', hop_length=1024)\n",
    "\n",
    "    ymax, ymin = 22000, 0\n",
    "    plt.vlines(target_onsets, ymin=ymin, ymax=ymax, colors='g', linestyles='--', linewidths=3)  # plot the signal\n",
    "    plt.vlines(segment_starts_in_s, ymax=ymax, ymin=ymin, colors='gray', linestyles='--', linewidths=1)  # Plot the segment lines\n",
    "    plt.vlines(pred_onsets, ymax=ymax, ymin=ymin, colors='r', linestyles='--', linewidths=2)  # Plot the pred onset lines\n",
    "\n",
    "\n",
    "def plot_preds_gt(batch, pred_onsets, window_size=1024, figsize=(40, 15)):\n",
    "    # get the data\n",
    "    signal = batch['signal'][0].data.cpu().numpy().ravel()\n",
    "    sr = batch['sr'].data.numpy()[0]\n",
    "    spec = batch['features'][0].data.cpu().numpy().squeeze()\n",
    "    target_onsets = batch['onsets'][0].data.cpu().numpy().squeeze()\n",
    "\n",
    "    visualize_predictions(signal, sr, spec, target_onsets, pred_onsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000000\n",
    "val_every = 1000\n",
    "save_every = 10000\n",
    "n_val = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "alpha = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while epoch < n_epochs:\n",
    "    for i_batch, train_batch in tqdm(enumerate(dataloader_train)):\n",
    "        iteration += 1\n",
    "\n",
    "        # predict\n",
    "        pred_labels, train_results = predict_and_evalaute(train_batch, model, onset_loss_func, notes_loss_func, DEVICE)\n",
    "        onset_loss, notes_loss = train_results.onset_loss, train_results.notes_loss\n",
    "        loss = alpha * onset_loss + notes_loss\n",
    "        \n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar('loss.train', loss.data.cpu().numpy(), iteration)\n",
    "        writer.add_scalar('loss.onset.train', onset_loss, iteration)\n",
    "        writer.add_scalar('loss.notes.train', notes_loss, iteration)\n",
    "\n",
    "        writer.add_scalar('acc.precision.train', train_results.precision, iteration)\n",
    "        writer.add_scalar('acc.recall.train', train_results.recall, iteration)\n",
    "\n",
    "        if iteration % val_every == 0:\n",
    "            val_onset_loss, val_notes_loss = 0, 0\n",
    "            average_precision, average_recall = 0, 0\n",
    "            for ix, val_batch in enumerate(dataloader_val):\n",
    "                _, results = predict_and_evalaute(val_batch, model, onset_loss_func, notes_loss_func, DEVICE, visualize=ix<2)\n",
    "                val_onset_loss += results.onset_loss.data.cpu().numpy()\n",
    "                val_notes_loss += results.notes_loss.data.cpu().numpy()\n",
    "\n",
    "                average_precision += results.precision\n",
    "                average_recall += results.recall\n",
    "            \n",
    "            # average out over all the batches\n",
    "            val_onset_loss, val_notes_loss = val_onset_loss / (ix + 1), val_notes_loss / (ix + 1)\n",
    "\n",
    "            # log!\n",
    "            writer.add_scalar('loss.val', alpha * val_onset_loss + val_notes_loss, iteration)\n",
    "            writer.add_scalar('loss.onset.val', val_onset_loss, iteration)\n",
    "            writer.add_scalar('loss.notes.val', val_notes_loss, iteration)\n",
    "\n",
    "            writer.add_scalar('acc.precision.val', average_precision / (ix + 1), iteration)\n",
    "            writer.add_scalar('acc.recall.val', average_recall / (ix + 1), iteration)\n",
    "            plt.show()\n",
    "            \n",
    "        if iteration % save_every == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(weights_folder, '{}.pt'.format(iteration)))\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch, iteration, i_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000000000\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
